{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c73413",
   "metadata": {},
   "source": [
    "üåå ZTF Dataset Column Definitions\n",
    "üî≠ 1. ra\n",
    "\n",
    "Right Ascension (in degrees) of the detection/object.\n",
    "\n",
    "Equivalent to longitude on the sky.\n",
    "\n",
    "Range: 0¬∞ to 360¬∞.\n",
    "\n",
    "üî≠ 2. dec\n",
    "\n",
    "Declination (in degrees)\n",
    "\n",
    "Equivalent to latitude on the sky.\n",
    "\n",
    "Range: ‚Äì90¬∞ to +90¬∞.\n",
    "\n",
    "üß† 3. infobits\n",
    "\n",
    "A bitwise flag indicating processing information about the exposure.\n",
    "\n",
    "Each bit represents a specific condition (e.g., read noise issue, saturated pixels, etc.).\n",
    "\n",
    "Typical values: 0 (no issues) or powers of 2 like 67108864.\n",
    "\n",
    "üó∫ 4. field\n",
    "\n",
    "ZTF Field ID\n",
    "\n",
    "ZTF sky is divided into 4096 square fields.\n",
    "\n",
    "This tells you which sky field the exposure belongs to.\n",
    "\n",
    "Example:\n",
    "field = 570 ‚Üí It is from ZTF Field 570.\n",
    "\n",
    "üì∑ 5. ccdid\n",
    "\n",
    "The ZTF camera has 16 CCDs, numbered:\n",
    "\n",
    "1 ‚Üí 16\n",
    "\n",
    "Represents which CCD captured this image.\n",
    "\n",
    "üì∑ 6. qid\n",
    "\n",
    "Each CCD has quadrants (sub-detectors):\n",
    "\n",
    "QID = 1, 2, 3, or 4\n",
    "\n",
    "Each corresponds to one readout quadrant.\n",
    "\n",
    "üì∑ 7. rcid\n",
    "\n",
    "Readout Channel ID (0‚Äì63)\n",
    "\n",
    "ZTF camera has 64 channels, each corresponds to a unique CCD quadrant.\n",
    "\n",
    "rcid = (CCDid - 1) √ó 4 + (QID - 1)\n",
    "\n",
    "Example:\n",
    "CCDid = 16, QID = 1 ‚Üí rcid = 60.\n",
    "\n",
    "Matches your data:\n",
    "ccdid = 16, qid = 1 ‚Üí rcid = 60\n",
    "\n",
    "üé® 8. fid\n",
    "\n",
    "Filter ID:\n",
    "\n",
    "fid\tFilter\n",
    "1\tzg (green)\n",
    "2\tzr (red)\n",
    "3\tzi (infrared)\n",
    "\n",
    "In the data:\n",
    "fid = 2 ‚Üí zr-band (red).\n",
    "\n",
    "üé® 9. filtercode\n",
    "\n",
    "The name of the filter used:\n",
    "\n",
    "‚Äúzg‚Äù\n",
    "\n",
    "‚Äúzr‚Äù\n",
    "\n",
    "‚Äúzi‚Äù\n",
    "\n",
    "Matches fid.\n",
    "\n",
    "üÜî 10. pid\n",
    "\n",
    "Processing ID (unique integer)\n",
    "\n",
    "A unique ID associated with the image/exposure\n",
    "\n",
    "Higher PID = more recent observation.\n",
    "\n",
    "üó∫ 11‚Äì18. ra1, dec1, ‚Ä¶ ra4, dec4\n",
    "\n",
    "These are the Four Corners of the CCD image footprint.\n",
    "\n",
    "ZTF stores the sky coordinates of the 4 corners of each CCD/quadrant image:\n",
    "\n",
    "Column\tMeaning\n",
    "ra1, dec1\tCorner 1 of the image\n",
    "ra2, dec2\tCorner 2\n",
    "ra3, dec3\tCorner 3\n",
    "ra4, dec4\tCorner 4\n",
    "\n",
    "These are used for:\n",
    "\n",
    "Mapping CCD footprint on the sky\n",
    "\n",
    "WCS calculations\n",
    "\n",
    "Checking if a target falls inside the image\n",
    "\n",
    "Astrometric corrections\n",
    "\n",
    "Example from your row:\n",
    "\n",
    "ra1 = 142.9726¬∞, dec1 = 22.6797¬∞\n",
    "\n",
    "ra2 = 142.0367¬∞, dec2 = 22.6647¬∞\n",
    "\n",
    "‚Ä¶ and so on\n",
    "\n",
    "Each set forms a quadrilateral outlining the image.\n",
    "\n",
    "üìÖ 19. ipac_pub_date\n",
    "\n",
    "The date the exposure was published to IRSA (not observation date).\n",
    "\n",
    "Example:\n",
    "2020-12-09 00:00:00+00\n",
    "\n",
    "üî¢ 20. ipac_gid\n",
    "\n",
    "Group ID used internally by IPAC.\n",
    "\n",
    "Usually values: 1‚Äì10\n",
    "\n",
    "Meaning:\n",
    "\n",
    "Grouping of exposures related to same night or processing batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb1860",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Loading and Inspecting the Raw ZTF Dataset</summary>\n",
    "\n",
    "This cell loads the previously downloaded ZTF search results into a DataFrame so the data can be inspected and used for further processing. It reads the CSV file generated from the ZTF image search, displays the first few rows, and prints the number of rows and columns in the dataset. This quick check helps confirm that the data was loaded correctly, contains the expected fields, and is ready for cleaning, selection, and analysis in the later steps of the research workflow. By previewing the structure of the dataset, the researcher gets an initial understanding of the metadata available, such as observation IDs, filters, timestamps, and file paths.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Reads the ZTF dataset** from a CSV file created during the image search step.  \n",
    "- **Loads the data into a Pandas DataFrame**, which is the standard tool for working with tabular datasets in data science.  \n",
    "- **Displays the first few rows** (`df.head()`), allowing the researcher to visually confirm what type of information the dataset contains.  \n",
    "- **Prints the dataset size** (number of rows and columns), helping to understand how large the dataset is before preprocessing.  \n",
    "- **Ensures the loaded data is correct** before moving on to cleaning, filtering, or extracting useful metadata.  \n",
    "- **Serves as the ‚Äústarting point‚Äù** for all subsequent data science operations such as feature engineering, anomaly detection, visualizations, and model training.  \n",
    "- **Helps verify the file path and content**, avoiding errors that could appear later if the dataset was not loaded properly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Matters for the Research**\n",
    "This step confirms that the astronomical metadata needed for the research is successfully captured in a structured format. Since the entire pipeline‚Äîincluding feature extraction, visualization, and modelling‚Äîdepends on this dataset, it is essential to verify the data early. By previewing and summarizing the dataset, the researcher ensures that the upcoming steps in the workflow are operating on valid ZTF data.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb56b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ra        dec  infobits  field  ccdid  qid  rcid  fid filtercode  \\\n",
      "0  142.513076  22.239831  67108864    570     16    1    60    2         zr   \n",
      "1  141.637575  21.360661         0    570     16    3    62    2         zr   \n",
      "2  141.618681  22.225694         0    570     16    2    61    2         zr   \n",
      "3  141.596498  22.204009  67108912    570     16    2    61    1         zg   \n",
      "4  141.637182  19.443859         0    570     12    3    46    2         zr   \n",
      "\n",
      "             pid  ...         ra1       dec1         ra2       dec2  \\\n",
      "0   769412526015  ...  142.972615  22.679692  142.036715  22.664666   \n",
      "1  1503405246215  ...  142.091906  21.803041  141.161603  21.782992   \n",
      "2  1504338726115  ...  142.075514  22.668183  141.139773  22.647755   \n",
      "3  1066544346115  ...  142.053404  22.646507  141.117659  22.626244   \n",
      "4  1521276364615  ...  142.085851  19.886330  141.167231  19.866253   \n",
      "\n",
      "          ra3       dec3         ra4       dec4           ipac_pub_date  \\\n",
      "0  142.055997  21.798581  142.986538  21.813558  2020-12-09 00:00:00+00   \n",
      "1  141.186132  20.917351  142.110832  20.937013  2022-09-07 00:00:00+00   \n",
      "2  141.164682  21.781999  142.094901  21.802092  2022-09-07 00:00:00+00   \n",
      "3  141.142411  21.760423  142.072649  21.780266  2020-06-24 00:00:00+00   \n",
      "4  141.191126  19.000577  142.104704  19.020278  2022-09-07 00:00:00+00   \n",
      "\n",
      "   ipac_gid  \n",
      "0         2  \n",
      "1         2  \n",
      "2         2  \n",
      "3         1  \n",
      "4         3  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Rows, Columns: (82955, 42)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load into a DataFrame\n",
    "df = pd.read_csv('ztf_image_search_results_full.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Summary of the raw dataset\n",
    "print('Rows, Columns:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f7ea9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Checking Missing Values, Normalizing Data, and Removing Duplicates</summary>\n",
    "\n",
    "This cell performs the essential early-stage data cleaning steps required to ensure the ZTF metadata is trustworthy and ready for analysis. It begins by identifying the data types of the columns and counting how many missing values exist in each one. Since astronomical datasets often contain different placeholder formats for missing information (such as ‚ÄúNA‚Äù, ‚ÄúNone‚Äù, or empty strings), the cell standardizes all these placeholders into proper `NaN` values. This normalization helps Pandas handle missing data consistently throughout the research.\n",
    "\n",
    "After cleaning missing-value formats, the cell removes any duplicate rows from the dataset. Duplicates can occur when multiple searches return overlapping results or when different query filters produce repeated entries. Removing duplicates ensures that later analyses‚Äîsuch as statistical summaries, visualizations, or machine-learning tasks‚Äîare not biased or influenced by repeated observations. The cell also prints how many duplicates were removed, giving the researcher a clear understanding of the dataset‚Äôs reliability before proceeding to deeper analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Counts missing values** in each column to understand the overall completeness of the dataset.  \n",
    "- **Detects different placeholder formats** used by the original data source (e.g., ‚ÄúNA‚Äù, ‚ÄúNone‚Äù, empty spaces).  \n",
    "- **Standardizes all placeholder values to `NaN`**, allowing Pandas to treat them correctly during feature engineering and modelling.  \n",
    "- **Improves data consistency**, which is crucial because even a few wrongly formatted entries can break later calculations.  \n",
    "- **Removes duplicate rows**, ensuring each astronomical observation appears only once.  \n",
    "- **Prevents model bias**, since duplicated records could artificially inflate certain classes or features.  \n",
    "- **Prints the number of removed duplicates**, confirming that the dataset has been cleaned successfully.  \n",
    "- **Forms a clean and reliable foundation** for every upcoming step, including visualization, feature extraction, and ML modelling.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical datasets like ZTF metadata often contain missing entries, inconsistent formatting, and duplicate records. If these issues are not corrected early, they can distort statistical patterns, weaken machine-learning performance, and mislead scientific conclusions. This cell establishes a clean, standardized dataset that ensures any insights, visualizations, or models created later in the research are accurate and trustworthy. By normalizing missing values and removing duplicates, the analysis becomes more robust and scientifically reliable.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00eca390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column types:\n",
      "float64    25\n",
      "int64      14\n",
      "object      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nColumn types:')\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288207f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "ra               0\n",
      "dec              0\n",
      "infobits         0\n",
      "field            0\n",
      "ccdid            0\n",
      "qid              0\n",
      "rcid             0\n",
      "fid              0\n",
      "filtercode       0\n",
      "pid              0\n",
      "nid              0\n",
      "expid            0\n",
      "itid             0\n",
      "imgtype          0\n",
      "imgtypecode      0\n",
      "obsdate          0\n",
      "obsjd            0\n",
      "exptime          0\n",
      "filefracday      0\n",
      "seeing           0\n",
      "airmass          0\n",
      "moonillf         0\n",
      "moonesb          0\n",
      "maglimit         0\n",
      "crpix1           0\n",
      "crpix2           0\n",
      "crval1           0\n",
      "crval2           0\n",
      "cd11             0\n",
      "cd12             0\n",
      "cd21             0\n",
      "cd22             0\n",
      "ra1              0\n",
      "dec1             0\n",
      "ra2              0\n",
      "dec2             0\n",
      "ra3              0\n",
      "dec3             0\n",
      "ra4              0\n",
      "dec4             0\n",
      "ipac_pub_date    0\n",
      "ipac_gid         0\n",
      "log1p_exptime    0\n",
      "log1p_airmass    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nMissing values per column:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30995f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After normalization of placeholders, missing per column:\n",
      "ra               0\n",
      "dec              0\n",
      "infobits         0\n",
      "field            0\n",
      "ccdid            0\n",
      "qid              0\n",
      "rcid             0\n",
      "fid              0\n",
      "filtercode       0\n",
      "pid              0\n",
      "nid              0\n",
      "expid            0\n",
      "itid             0\n",
      "imgtype          0\n",
      "imgtypecode      0\n",
      "obsdate          0\n",
      "obsjd            0\n",
      "exptime          0\n",
      "filefracday      0\n",
      "seeing           0\n",
      "airmass          0\n",
      "moonillf         0\n",
      "moonesb          0\n",
      "maglimit         0\n",
      "crpix1           0\n",
      "crpix2           0\n",
      "crval1           0\n",
      "crval2           0\n",
      "cd11             0\n",
      "cd12             0\n",
      "cd21             0\n",
      "cd22             0\n",
      "ra1              0\n",
      "dec1             0\n",
      "ra2              0\n",
      "dec2             0\n",
      "ra3              0\n",
      "dec3             0\n",
      "ra4              0\n",
      "dec4             0\n",
      "ipac_pub_date    0\n",
      "ipac_gid         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace common text placeholders with NaN for consistent handling\n",
    "import numpy as np\n",
    "df.replace(['', ' ', 'NA', 'NaN', 'nan', 'None', 'none', 'NULL'], np.nan, inplace=True)\n",
    "print('\\nAfter normalization of placeholders, missing per column:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35115fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# 2) Remove duplicate rows\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "after = df.shape[0]\n",
    "print(f'Removed {before - after} duplicate rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de03fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Handling Missing Data Using a Structured Cleaning Strategy</summary>\n",
    "\n",
    "This cell applies a systematic method to clean missing values from the ZTF dataset. Real astronomical datasets often contain gaps‚Äîsome observations may be incomplete, corrupted, or missing certain metadata fields. Instead of removing all rows with missing values (which would waste valuable data), this cell uses a balanced strategy to preserve as much information as possible while still ensuring dataset quality.\n",
    "\n",
    "The process begins by removing only those rows that are mostly empty (more than 50% missing values). This avoids keeping rows that contain almost no useful information. After that, the cell separates numerical and categorical columns because each type requires a different method for filling missing values. Numerical columns‚Äîsuch as flux values, coordinates, or exposure metadata‚Äîare filled using the **median**, which is a stable and reliable measure that reduces the influence of outliers. Categorical columns‚Äîsuch as file names, filter types, or detector IDs‚Äîare filled using the **mode**, which replaces missing values with the most common category.\n",
    "\n",
    "This cleaning strategy ensures that the dataset remains as complete as possible without introducing bias. The final print statement confirms how many missing values remain in each column after the cleaning process, ensuring transparency and reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Removes only highly incomplete rows** (rows missing more than 50% of their values), preventing unnecessary data loss.  \n",
    "- **Separates numerical and categorical columns**, because each type requires different imputation methods.  \n",
    "- **Fills missing numeric values with the median**, which is robust and prevents distortion from extreme outliers.  \n",
    "- **Replaces missing categorical values with the mode**, ensuring the filled value matches the most common or expected category.  \n",
    "- **Uses a fallback option (‚ÄòUnknown‚Äô)** if no mode exists, making the dataset consistent.  \n",
    "- **Ensures the dataset has no remaining missing values** that could break future analysis steps.  \n",
    "- **Keeps the data scientifically meaningful**, since the strategy preserves important observations instead of discarding them.  \n",
    "- **Prepares the dataset for machine learning**, where models require complete and clean data to perform accurately.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical data frequently comes with missing fields due to observation limits, sensor errors, or data transmission issues. Poor handling of missing values can lead to biased models, incorrect scientific insights, or unstable algorithms. This cell applies a well-established and research-friendly cleaning strategy that maintains the integrity and usefulness of the ZTF dataset. By filling gaps intelligently and removing only severely incomplete records, the dataset becomes reliable, consistent, and ready for deeper statistical analysis, visualization, and machine-learning workflows.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4c6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After null handling, remaining missing per column:\n",
      "ra               0\n",
      "dec              0\n",
      "infobits         0\n",
      "field            0\n",
      "ccdid            0\n",
      "qid              0\n",
      "rcid             0\n",
      "fid              0\n",
      "filtercode       0\n",
      "pid              0\n",
      "nid              0\n",
      "expid            0\n",
      "itid             0\n",
      "imgtype          0\n",
      "imgtypecode      0\n",
      "obsdate          0\n",
      "obsjd            0\n",
      "exptime          0\n",
      "filefracday      0\n",
      "seeing           0\n",
      "airmass          0\n",
      "moonillf         0\n",
      "moonesb          0\n",
      "maglimit         0\n",
      "crpix1           0\n",
      "crpix2           0\n",
      "crval1           0\n",
      "crval2           0\n",
      "cd11             0\n",
      "cd12             0\n",
      "cd21             0\n",
      "cd22             0\n",
      "ra1              0\n",
      "dec1             0\n",
      "ra2              0\n",
      "dec2             0\n",
      "ra3              0\n",
      "dec3             0\n",
      "ra4              0\n",
      "dec4             0\n",
      "ipac_pub_date    0\n",
      "ipac_gid         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3) Remove nulls with strategy: drop rows with >50% missing, impute numerics with median and categoricals with mode\n",
    "threshold = int(df.shape[1] * 0.5)\n",
    "df = df.dropna(thresh=threshold).reset_index(drop=True)\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for c in num_cols:\n",
    "    if df[c].isnull().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "for c in cat_cols:\n",
    "    if df[c].isnull().any():\n",
    "        mode = df[c].mode()\n",
    "        fill_val = mode.iloc[0] if not mode.empty else 'Unknown'\n",
    "        df[c] = df[c].fillna(fill_val)\n",
    "print('After null handling, remaining missing per column:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da2e74",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Outlier Removal Using the IQR Method</summary>\n",
    "\n",
    "This cell removes numerical outliers from the dataset using a well-known statistical technique called the Interquartile Range (IQR) method. Outliers are unusually large or small values that do not fit the normal pattern of the data. In astronomy datasets such as ZTF metadata, outliers can occur due to sensor noise, faulty readings, extreme environmental conditions, or rare technical errors during observations. If left uncorrected, these extreme values can distort graphs, shift averages, mislead machine-learning models, and produce unstable scientific conclusions.\n",
    "\n",
    "The cell identifies all numerical columns and applies the IQR rule to each one. It calculates the first quartile (Q1), the third quartile (Q3), and the IQR (Q3 ‚àí Q1). Any values falling outside the acceptable range (Q1 ‚àí 1.5√óIQR to Q3 + 1.5√óIQR) are considered outliers. The code removes these rows in a cumulative manner, meaning that once outliers from one column are removed, the next column is checked on the remaining dataset. A final print statement shows how many total rows were removed. This ensures that the cleaned dataset contains realistic and scientifically reliable values that improve the accuracy and stability of later analyses.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Targets numerical columns only**, since outliers mainly occur in measurements rather than text fields.  \n",
    "- **Uses the IQR rule**, a widely accepted and robust method for detecting extreme values.  \n",
    "- **Calculates Q1, Q3, and IQR** to understand the typical spread of each numerical feature.  \n",
    "- **Marks values outside 1.5√óIQR as outliers**, which is a standard threshold used in statistics.  \n",
    "- **Removes outliers iteratively**, ensuring each column is cleaned based on the updated dataset.  \n",
    "- **Protects the scientific validity** of the dataset by removing physically unrealistic or faulty measurements.  \n",
    "- **Prevents machine-learning models from being influenced by incorrect values**, improving prediction stability and accuracy.  \n",
    "- **Displays how many rows were removed**, providing transparency in the data-cleaning process.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical datasets often contain unexpected spikes or errors due to equipment limitations, noise, atmospheric disturbances, or data corruption. These outliers can strongly influence trends, distort visualizations, and misguide the learning behaviour of machine-learning models. By removing unrealistic values using a mathematically sound method, the dataset becomes smoother, cleaner, and far more reliable. This step ensures that all future analysis‚Äîwhether visualization, feature extraction, or modelling‚Äîis based on high-quality and trustworthy astronomical data.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d60c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns to check for outliers: ['ra', 'dec', 'infobits', 'field', 'ccdid', 'qid', 'rcid', 'fid', 'pid', 'nid', 'expid', 'itid', 'obsjd', 'exptime', 'filefracday', 'seeing', 'airmass', 'moonillf', 'moonesb', 'maglimit', 'crpix1', 'crpix2', 'crval1', 'crval2', 'cd11', 'cd12', 'cd21', 'cd22', 'ra1', 'dec1', 'ra2', 'dec2', 'ra3', 'dec3', 'ra4', 'dec4', 'ipac_gid']\n",
      "Removed 20587 rows as outliers (cumulative)\n"
     ]
    }
   ],
   "source": [
    "# 4) Remove outliers using IQR on numeric columns (iterative cumulative removal)\n",
    "import pandas as pd\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "print('Numeric columns to check for outliers:', num_cols)\n",
    "initial_rows = df.shape[0]\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    if pd.isnull(IQR) or IQR == 0:\n",
    "        continue\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    df = df[(df[col] >= lower) & (df[col] <= upper)].reset_index(drop=True)\n",
    "print(f'Removed {initial_rows - df.shape[0]} rows as outliers (cumulative)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5d1a6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Removing Astronomically Impossible or Invalid Values</summary>\n",
    "\n",
    "This cell performs domain-specific cleaning by removing values that are scientifically impossible in astronomy or clearly incorrect. While previous cleaning steps handled missing values and statistical outliers, this step ensures that all remaining data follows the real physical rules of the sky. Astronomical coordinates such as Right Ascension (RA) and Declination (Dec) have strict valid ranges. Any value outside these limits indicates an error in measurement or metadata. The cell also removes infinite values, NaN entries, and ensures that measurement-related columns, such as flux error, contain only physically meaningful positive values.\n",
    "\n",
    "By applying these astronomy-based filters, the dataset becomes scientifically trustworthy. This is essential because machine-learning models trained on physically impossible values may produce misleading predictions. The cell also prints how many rows were removed for each check, improving transparency. This final cleaning stage ensures that the dataset is ready for scientific analysis, visualization, and model training without containing physically unrealistic values.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Validates Right Ascension (RA)**  \n",
    "  - RA must be between **0¬∞ and 360¬∞**.  \n",
    "  - Any value outside this range is physically impossible and therefore removed.\n",
    "\n",
    "- **Validates Declination (Dec)**  \n",
    "  - Dec must lie between **‚Äì90¬∞ and +90¬∞**, matching the limits of the celestial sphere.  \n",
    "  - Incorrect readings are filtered out.\n",
    "\n",
    "- **Removes infinite values**  \n",
    "  - Infinite or undefined numerical values often occur due to division errors or corrupted entries.  \n",
    "  - These are replaced with NaN and then removed entirely.\n",
    "\n",
    "- **Drops remaining NaN values**  \n",
    "  - Ensures the final dataset contains no missing or undefined measurements.\n",
    "\n",
    "- **Checks flux error values**  \n",
    "  - If `flux` and `flux_err` exist:  \n",
    "    - flux error must be **positive**.  \n",
    "    - Non-positive values usually indicate sensor faults or improperly processed data.\n",
    "\n",
    "- **Ensures scientific correctness**  \n",
    "  - Keeps only physically meaningful astronomical measurements.  \n",
    "  - Removes corrupted or impossible metadata before modelling.\n",
    "\n",
    "- **Improves reliability of ML models**  \n",
    "  - Prevents machine-learning algorithms from learning incorrect or physically meaningless patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical datasets can contain values that violate the laws of the sky due to sensor glitches, transmission errors, or preprocessing issues. If such values remain in the dataset, they can mislead scientific interpretations or cause machine-learning models to behave unpredictably. By applying physical limits (like RA and Dec ranges) and removing invalid flux measurements, this cell ensures that the dataset respects real astronomical constraints.\n",
    "\n",
    "This domain-aware cleaning step is what transforms a raw telescope dataset into a **scientifically valid dataset**, ready for trustworthy analysis and modelling.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b9603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RA outside [0,360], removed 0\n",
      "Filtered Dec outside [-90,90], removed 0\n",
      "After removing infinite/NaN values, rows: 62368\n"
     ]
    }
   ],
   "source": [
    "# 5) Remove incorrect / impossible values (example astronomical checks)\n",
    "import numpy as np\n",
    "if 'ra' in df.columns:\n",
    "    before = df.shape[0]\n",
    "    df = df[(df['ra'] >= 0) & (df['ra'] <= 360)].reset_index(drop=True)\n",
    "    print('Filtered RA outside [0,360], removed', before - df.shape[0])\n",
    "if 'dec' in df.columns:\n",
    "    before = df.shape[0]\n",
    "    df = df[(df['dec'] >= -90) & (df['dec'] <= 90)].reset_index(drop=True)\n",
    "    print('Filtered Dec outside [-90,90], removed', before - df.shape[0])\n",
    "# Replace infinities and drop any remaining NaNs\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna().reset_index(drop=True)\n",
    "print('After removing infinite/NaN values, rows:', df.shape[0])\n",
    "# Example: require positive flux_err if those columns exist\n",
    "if set(['flux', 'flux_err']).issubset(df.columns):\n",
    "    before = df.shape[0]\n",
    "    df = df[df['flux_err'] > 0].reset_index(drop=True)\n",
    "    print('Removed rows with non-positive flux_err:', before - df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ecd4e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Standardizing Numerical Features for Machine-Learning</summary>\n",
    "\n",
    "This cell prepares the cleaned astronomical dataset for machine-learning by applying **standardization**, a common scaling technique used in data science. After cleaning invalid values, the dataset may still contain numerical columns with very different units or scales (for example, flux values vs. pixel positions vs. exposure times). Machine-learning models can become unstable or biased if these differences are not handled properly. Standardization transforms all selected numerical columns so that they share a similar scale, making the dataset mathematically well-behaved for algorithms that rely on distance, gradients, or statistical assumptions.\n",
    "\n",
    "The cell identifies usable numerical columns, excluding ID fields, filenames, and time-related columns that should not be scaled. It then applies `StandardScaler`, which centers the data around a mean of 0 and a standard deviation of 1. The scaled version of the dataset is stored in a new DataFrame called `df_standardized`, ensuring that the original values remain intact for reference or other types of analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Identifies numerical columns suitable for scaling**, ignoring IDs, names, and timestamps because these fields do not represent measurable quantities.  \n",
    "- **Avoids scaling inappropriate features**, such as filenames or observation dates, which would distort their meaning.  \n",
    "- **Uses Standardization**:  \n",
    "  - Converts each selected feature to have a **mean of 0** and **standard deviation of 1**.  \n",
    "  - Helps algorithms treat all numerical features fairly.  \n",
    "- **Creates a new DataFrame (`df_standardized`)** containing the standardized version of the dataset.  \n",
    "- **Prevents model bias**, since unscaled features with large numeric ranges could overpower smaller-scaled features.  \n",
    "- **Ensures mathematical stability** during machine-learning tasks, especially for distance-based or gradient-based models.  \n",
    "- **Prints the columns being scaled**, offering transparency and reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical datasets often contain measurements with extremely different numerical ranges‚Äîfor example, flux values, exposure metadata, geometric positions, and detector readouts. Machine-learning models become more accurate, balanced, and reliable when all these values are brought to a common scale. Standardization is therefore a crucial preprocessing step before performing classification, clustering, anomaly detection, or dimensionality reduction. By preserving both the original and standardized versions of the dataset, this cell supports flexible experimentation while ensuring scientific rigor.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d2ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to scale: ['ra', 'dec', 'infobits', 'field', 'filefracday', 'seeing', 'airmass', 'moonillf', 'moonesb', 'maglimit', 'crpix1', 'crpix2', 'crval1', 'crval2', 'cd11', 'cd12', 'cd21', 'cd22', 'ra1', 'dec1', 'ra2', 'dec2', 'ra3', 'dec3', 'ra4', 'dec4']\n",
      "Standardization complete. Dataframe `df_standardized` is available.\n"
     ]
    }
   ],
   "source": [
    "# 6) Scaling: Standardization (store standardized copy)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "exclude_patterns = ['id', 'Id', 'ID', 'name', 'filename', 'time', 'date', 'jd']\n",
    "exclude_cols = [c for c in df.columns for p in exclude_patterns if p in c]\n",
    "cols_to_scale = [c for c in numeric_cols if c not in exclude_cols]\n",
    "print('Columns to scale:', cols_to_scale)\n",
    "scaler = StandardScaler()\n",
    "if cols_to_scale:\n",
    "    df_standardized = df.copy()\n",
    "    df_standardized[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    print('Standardization complete. Dataframe `df_standardized` is available.')\n",
    "else:\n",
    "    print('No numeric columns found to scale.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c2955",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Feature Engineering to Create New Useful Scientific Features</summary>\n",
    "\n",
    "This cell adds new derived features to the dataset to make the astronomical data more meaningful and informative for analysis and machine-learning. Feature engineering is the process of creating new variables from existing ones so that the model can better understand underlying scientific patterns. In astronomy, raw measurements alone are often not enough‚Äîderived quantities such as signal-to-noise ratio or logarithmic transformations reveal hidden trends that help models interpret faint objects, noisy measurements, and time-based behavior more effectively.\n",
    "\n",
    "The cell first creates a **signal-to-noise ratio (SNR)** feature, which is critical in astronomy because it measures how strong a signal (flux) is relative to uncertainty (flux error). A higher SNR means a more reliable detection. Then, the cell searches for any possible time-related column (e.g., date, time, JD, MJD) and extracts the year, month, and day from it. This allows the dataset to capture temporal patterns, such as seasonal observation differences or instrument behavior over time.\n",
    "\n",
    "Finally, for any numerical column that is strictly positive and strongly skewed, the cell creates a **logarithmic transformation** (`log1p_`), which reduces extreme values and makes distributions more balanced. Balanced distributions help machine-learning models learn more consistently. The cell prints messages for each new feature created, ensuring transparency.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Creates SNR feature (`snr = flux / flux_err`)**  \n",
    "  - A fundamental astronomy metric indicating data quality and detection reliability.  \n",
    "  - Higher SNR = stronger, clearer astronomical signal.\n",
    "\n",
    "- **Automatically detects time/date columns**  \n",
    "  - Searches for patterns like ‚Äúdate‚Äù, ‚Äútime‚Äù, ‚ÄúJD‚Äù, ‚ÄúMJD‚Äù.  \n",
    "  - Useful because not all datasets name their time columns consistently.\n",
    "\n",
    "- **Extracts year, month, and day**  \n",
    "  - Helps analyze trends across years, months, or nights of observation.  \n",
    "  - Time-based features can improve classification and temporal modelling.\n",
    "\n",
    "- **Applies logarithmic transformation to skewed positive features**  \n",
    "  - Makes extremely large values more manageable.  \n",
    "  - Reduces the effect of extreme outliers in heavily skewed astrophysical distributions.  \n",
    "  - Adds new columns named `log1p_columnName`.\n",
    "\n",
    "- **Ensures feature engineering is dynamic and adaptive**  \n",
    "  - Only applies transformations when conditions are appropriate.  \n",
    "  - Prevents accidental modification of unsuitable columns.\n",
    "\n",
    "- **Improves model learning**  \n",
    "  - Derived features help capture physical relationships that raw measurements alone cannot reveal.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical measurements often contain noise, extreme values, and hidden patterns that are difficult to detect without transforming the raw data. By generating features such as SNR, time components, and logarithmic versions of skewed measurements, the dataset becomes far richer and more expressive. These engineered features help machine-learning models better interpret celestial behavior, improve prediction accuracy, and capture scientific relationships that are essential in astroinformatics. This step significantly enhances the dataset‚Äôs value and paves the way for more powerful and reliable modelling.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506c668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse date column obsdate  -  time data \"2018-03-25 06:35:35+00\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f%z\", at position 35. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "Created log1p_exptime\n",
      "Created log1p_airmass\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# 7) Feature engineering\n",
    "import numpy as np\n",
    "if set(['flux', 'flux_err']).issubset(df.columns):\n",
    "    df['snr'] = df['flux'] / df['flux_err']\n",
    "    print('Created `snr` feature')\n",
    "possible_date_cols = [c for c in df.columns if any(x in c.lower() for x in ['date','time','jd','mjd'])]\n",
    "if possible_date_cols:\n",
    "    date_col = possible_date_cols[0]\n",
    "    try:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['day'] = df[date_col].dt.day\n",
    "        print(f'Derived year/month/day from {date_col}')\n",
    "    except Exception as e:\n",
    "        print('Could not parse date column', date_col, ' - ', e)\n",
    "for c in df.select_dtypes(include=['number']).columns:\n",
    "    if (df[c] > 0).all() and df[c].skew() > 1:\n",
    "        df['log1p_' + c] = np.log1p(df[c])\n",
    "        print('Created log1p_' + c)\n",
    "print('Feature engineering complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e314819",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Handling Class Imbalance Using SMOTE (If a Target Column Exists)</summary>\n",
    "\n",
    "This cell checks whether the dataset contains a target or label column (such as ‚Äúclass‚Äù, ‚Äútype‚Äù, or ‚Äútarget‚Äù) and, if so, it attempts to correct **class imbalance** using SMOTE. Class imbalance happens when one category has many more samples than others. This is very common in astronomy‚Äîfor example, there may be thousands of normal observations but only a few rare events like supernovae or unusual transients. If this imbalance is not handled, machine-learning models tend to ignore the rare classes and perform poorly on the very objects scientists care most about.\n",
    "\n",
    "This cell automatically detects a target column, prints the current class distribution, and then applies **SMOTE (Synthetic Minority Oversampling Technique)**. SMOTE creates new synthetic examples for the minority classes by interpolating between existing observations. This helps balance the dataset without simply duplicating rows. The cell uses only numerical features for SMOTE, generates a new balanced dataset, and prints the final shape. If SMOTE cannot run (e.g., missing library or no numerical columns), the cell safely skips resampling and prints a clear message.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Automatically detects a target column**, such as ‚Äúlabel‚Äù, ‚Äúclass‚Äù, ‚Äútype‚Äù, or ‚Äútarget‚Äù.  \n",
    "- **Displays class counts before resampling**, giving a clear picture of how imbalanced the dataset is.  \n",
    "- **Uses SMOTE**, a widely used technique that:  \n",
    "  - Creates **synthetic samples** for minority classes.  \n",
    "  - Prevents models from being biased toward the majority class.  \n",
    "  - Improves fairness and predictive accuracy.  \n",
    "- **Uses only numerical features** to generate synthetic samples, as SMOTE requires numeric inputs.  \n",
    "- **Produces a new balanced dataset (`df_res`)** that can be used for machine-learning tasks.  \n",
    "- **Handles errors safely**‚Äîif SMOTE is unavailable or unsuitable, the code prints an informative message.  \n",
    "- **Keeps the research workflow flexible**, allowing the model to work with either the original or resampled dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Astronomical datasets often contain **rare events**, such as unusual transients or special categories of objects. These rare classes are usually the most scientifically interesting, but machine-learning models struggle to learn from them when the dataset is imbalanced. By applying SMOTE, the research ensures that minority categories receive equal representation during training. This leads to a more reliable, fair, and scientifically meaningful model that does not ignore rare events. Balancing the dataset is essential for achieving stable and accurate predictions in astroinformatics applications.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d580e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No obvious target column found; skipping imbalance handling\n"
     ]
    }
   ],
   "source": [
    "# 8) Handle class imbalance (try SMOTE if a target column exists)\n",
    "from collections import Counter\n",
    "possible_targets = [c for c in df.columns if c.lower() in ['label','class','target','type']]\n",
    "if possible_targets:\n",
    "    target = possible_targets[0]\n",
    "    print('Target column detected:', target)\n",
    "    print('Class counts before:', Counter(df[target]))\n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        features = df.drop(columns=[target]).select_dtypes(include=['number']).columns.tolist()\n",
    "        if features:\n",
    "            X = df[features]\n",
    "            y = df[target]\n",
    "            sm = SMOTE(random_state=42)\n",
    "            X_res, y_res = sm.fit_resample(X, y)\n",
    "            df_res = pd.DataFrame(X_res, columns=features)\n",
    "            df_res[target] = y_res\n",
    "            print('Resampled dataset shape:', df_res.shape)\n",
    "        else:\n",
    "            print('No numeric feature columns available for SMOTE; skipping resampling.')\n",
    "    except Exception as e:\n",
    "        print('SMOTE failed or imblearn not installed:', e)\n",
    "else:\n",
    "    print('No obvious target column found; skipping imbalance handling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2500b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Final Verification and Saving the Cleaned Datasets</summary>\n",
    "\n",
    "This cell performs the final confirmation steps in the cleaning pipeline and saves the fully processed datasets to disk. After all earlier stages‚Äîsuch as removing missing values, fixing invalid entries, handling outliers, engineering new features, and balancing classes‚Äîthe dataset is now clean, consistent, and ready for analysis or machine-learning. The cell prints the final shape of the dataset, displays the data types, and then exports the cleaned DataFrame into a new CSV file for future use. If a standardized version of the dataset was created earlier, this cell also saves that version separately. Finally, it previews the first few rows so the researcher can visually confirm that the dataset looks correct.\n",
    "\n",
    "This saving process is essential because it freezes the cleaned dataset in a stable and reproducible format. Future analysis, modelling, visualizations, or external collaborators can now use the same prepared dataset without rerunning the entire cleaning pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Verifies the final shape of the dataset**, ensuring no unexpected row or column changes occurred during cleaning.  \n",
    "- **Displays the final data types**, confirming that numerical and categorical columns were processed correctly.  \n",
    "- **Saves the cleaned dataset** as `ztf_image_search_results_full_cleaned.csv` for easy reuse.  \n",
    "- **Optionally saves the standardized dataset**, if it was created earlier, ensuring both raw-cleaned and scaled versions are available.  \n",
    "- **Provides a quick preview (`df.head()`)** so the researcher can visually inspect the final output.  \n",
    "- **Ensures reproducibility**, because future experiments can load the exact same dataset without repeating all cleaning steps.  \n",
    "- **Supports collaboration**, as other researchers or viva examiners can view the cleaned dataset directly.  \n",
    "- **Marks the completion of the full data-preprocessing pipeline**, allowing the research to move on to modelling, analysis, or visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "Saving the cleaned and standardized datasets makes the entire preprocessing workflow transparent and repeatable‚Äîtwo core requirements of scientific research. Machine-learning experiments depend on consistent input data. Without saving the processed dataset, even small variations in cleaning steps could produce different results. This cell ensures that the research builds on a stable and reliable foundation, making every analysis step that follows scientifically valid, verifiable, and easy to reproduce. It also provides a clean dataset that can be shared with supervisors, collaborators, or the viva evaluation panel.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0972eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (62368, 44)\n",
      "ra               float64\n",
      "dec              float64\n",
      "infobits           int64\n",
      "field              int64\n",
      "ccdid              int64\n",
      "qid                int64\n",
      "rcid               int64\n",
      "fid                int64\n",
      "filtercode        object\n",
      "pid                int64\n",
      "nid                int64\n",
      "expid              int64\n",
      "itid               int64\n",
      "imgtype           object\n",
      "imgtypecode       object\n",
      "obsdate           object\n",
      "obsjd            float64\n",
      "exptime            int64\n",
      "filefracday        int64\n",
      "seeing           float64\n",
      "airmass          float64\n",
      "moonillf         float64\n",
      "moonesb            int64\n",
      "maglimit         float64\n",
      "crpix1           float64\n",
      "crpix2           float64\n",
      "crval1           float64\n",
      "crval2           float64\n",
      "cd11             float64\n",
      "cd12             float64\n",
      "cd21             float64\n",
      "cd22             float64\n",
      "ra1              float64\n",
      "dec1             float64\n",
      "ra2              float64\n",
      "dec2             float64\n",
      "ra3              float64\n",
      "dec3             float64\n",
      "ra4              float64\n",
      "dec4             float64\n",
      "ipac_pub_date     object\n",
      "ipac_gid           int64\n",
      "log1p_exptime    float64\n",
      "log1p_airmass    float64\n",
      "dtype: object\n",
      "Cleaned dataset saved to ztf_image_search_results_full_cleaned.csv\n",
      "Standardized dataset saved to ztf_image_search_results_full_standardized.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>infobits</th>\n",
       "      <th>field</th>\n",
       "      <th>ccdid</th>\n",
       "      <th>qid</th>\n",
       "      <th>rcid</th>\n",
       "      <th>fid</th>\n",
       "      <th>filtercode</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>ra2</th>\n",
       "      <th>dec2</th>\n",
       "      <th>ra3</th>\n",
       "      <th>dec3</th>\n",
       "      <th>ra4</th>\n",
       "      <th>dec4</th>\n",
       "      <th>ipac_pub_date</th>\n",
       "      <th>ipac_gid</th>\n",
       "      <th>log1p_exptime</th>\n",
       "      <th>log1p_airmass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142.513076</td>\n",
       "      <td>22.239831</td>\n",
       "      <td>67108864</td>\n",
       "      <td>570</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>zr</td>\n",
       "      <td>769412526015</td>\n",
       "      <td>...</td>\n",
       "      <td>142.036715</td>\n",
       "      <td>22.664666</td>\n",
       "      <td>142.055997</td>\n",
       "      <td>21.798581</td>\n",
       "      <td>142.986538</td>\n",
       "      <td>21.813558</td>\n",
       "      <td>2020-12-09 00:00:00+00</td>\n",
       "      <td>2</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.752830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141.637575</td>\n",
       "      <td>21.360661</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>zr</td>\n",
       "      <td>1503405246215</td>\n",
       "      <td>...</td>\n",
       "      <td>141.161603</td>\n",
       "      <td>21.782992</td>\n",
       "      <td>141.186132</td>\n",
       "      <td>20.917351</td>\n",
       "      <td>142.110832</td>\n",
       "      <td>20.937013</td>\n",
       "      <td>2022-09-07 00:00:00+00</td>\n",
       "      <td>2</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.755183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.618681</td>\n",
       "      <td>22.225694</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>zr</td>\n",
       "      <td>1504338726115</td>\n",
       "      <td>...</td>\n",
       "      <td>141.139773</td>\n",
       "      <td>22.647755</td>\n",
       "      <td>141.164682</td>\n",
       "      <td>21.781999</td>\n",
       "      <td>142.094901</td>\n",
       "      <td>21.802092</td>\n",
       "      <td>2022-09-07 00:00:00+00</td>\n",
       "      <td>2</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.710004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141.637182</td>\n",
       "      <td>19.443859</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>zr</td>\n",
       "      <td>1521276364615</td>\n",
       "      <td>...</td>\n",
       "      <td>141.167231</td>\n",
       "      <td>19.866253</td>\n",
       "      <td>141.191126</td>\n",
       "      <td>19.000577</td>\n",
       "      <td>142.104704</td>\n",
       "      <td>19.020278</td>\n",
       "      <td>2022-09-07 00:00:00+00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.709513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141.637223</td>\n",
       "      <td>19.440343</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>zg</td>\n",
       "      <td>1519302464615</td>\n",
       "      <td>...</td>\n",
       "      <td>141.167275</td>\n",
       "      <td>19.862742</td>\n",
       "      <td>141.191122</td>\n",
       "      <td>18.996967</td>\n",
       "      <td>142.104734</td>\n",
       "      <td>19.016760</td>\n",
       "      <td>2021-06-30 00:00:00+00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.710496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ra        dec  infobits  field  ccdid  qid  rcid  fid filtercode  \\\n",
       "0  142.513076  22.239831  67108864    570     16    1    60    2         zr   \n",
       "1  141.637575  21.360661         0    570     16    3    62    2         zr   \n",
       "2  141.618681  22.225694         0    570     16    2    61    2         zr   \n",
       "3  141.637182  19.443859         0    570     12    3    46    2         zr   \n",
       "4  141.637223  19.440343         0    570     12    3    46    1         zg   \n",
       "\n",
       "             pid  ...         ra2       dec2         ra3       dec3  \\\n",
       "0   769412526015  ...  142.036715  22.664666  142.055997  21.798581   \n",
       "1  1503405246215  ...  141.161603  21.782992  141.186132  20.917351   \n",
       "2  1504338726115  ...  141.139773  22.647755  141.164682  21.781999   \n",
       "3  1521276364615  ...  141.167231  19.866253  141.191126  19.000577   \n",
       "4  1519302464615  ...  141.167275  19.862742  141.191122  18.996967   \n",
       "\n",
       "          ra4       dec4           ipac_pub_date  ipac_gid  log1p_exptime  \\\n",
       "0  142.986538  21.813558  2020-12-09 00:00:00+00         2       3.433987   \n",
       "1  142.110832  20.937013  2022-09-07 00:00:00+00         2       3.433987   \n",
       "2  142.094901  21.802092  2022-09-07 00:00:00+00         2       3.433987   \n",
       "3  142.104704  19.020278  2022-09-07 00:00:00+00         3       3.433987   \n",
       "4  142.104734  19.016760  2021-06-30 00:00:00+00         1       3.433987   \n",
       "\n",
       "   log1p_airmass  \n",
       "0       0.752830  \n",
       "1       0.755183  \n",
       "2       0.710004  \n",
       "3       0.709513  \n",
       "4       0.710496  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) Final checks and save cleaned data\n",
    "print('Final shape:', df.shape)\n",
    "print(df.dtypes)\n",
    "output_path = 'ztf_image_search_results_full_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print('Cleaned dataset saved to', output_path)\n",
    "try:\n",
    "    df_standardized.to_csv('ztf_image_search_results_full_standardized.csv', index=False)\n",
    "    print('Standardized dataset saved to ztf_image_search_results_full_standardized.csv')\n",
    "except NameError:\n",
    "    pass\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
