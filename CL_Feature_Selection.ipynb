{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9f4ae9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Loading the Standardized Dataset and Identifying Target & Astrophysical Feature Columns</summary>\n",
    "\n",
    "This cell begins the feature‚Äêselection and machine‚Äêlearning workflow by loading the **standardized ZTF dataset** and identifying which columns are most relevant for scientific and ML analysis. Standardization ensures that all numeric variables share a consistent scale, which is important for many statistical and machine-learning methods.\n",
    "\n",
    "Additionally, the cell imports the full set of feature-selection libraries and ML tools that will be used later, including Random Forest, Logistic Regression, Variance Threshold, Mutual Information, and KMeans clustering.\n",
    "\n",
    "To prepare the dataset for intelligent feature selection, the script scans the column names to automatically detect:\n",
    "\n",
    "- **Possible target variables** (labels for classification tasks)  \n",
    "- **Astrophysically meaningful features**, such as coordinates, brightness measurements, signal-to-noise ratio, observing conditions, and time information  \n",
    "\n",
    "This helps ensure that important scientific features are not mistakenly removed during feature-selection steps.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Loads the cleaned and standardized dataset**, ensuring all variables have comparable numeric scales.  \n",
    "- **Imports essential feature-selection tools**, including:\n",
    "  - VarianceThreshold  \n",
    "  - SelectKBest  \n",
    "  - Mutual Information  \n",
    "  - Random Forest  \n",
    "  - Logistic Regression  \n",
    "  - KMeans  \n",
    "- **Detects possible target/label columns**, such as:\n",
    "  - \"label\", \"class\", \"target\", \"type\"  \n",
    "  which may be used for supervised learning tasks.  \n",
    "- **Identifies astrophysical priority features**, such as:\n",
    "  - RA/Dec ‚Üí sky position  \n",
    "  - Flux/Magnitude ‚Üí brightness  \n",
    "  - SNR ‚Üí data quality  \n",
    "  - Seeing/Airmass ‚Üí atmospheric conditions  \n",
    "  - Filter/Band ‚Üí wavelength of observation  \n",
    "  - JD/MJD/ObsDate ‚Üí time of observation  \n",
    "- **Prints dataset shape and detected features**, confirming dataset readiness for ML workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "This block performs essential preparation steps before applying any feature-selection or learning algorithm:\n",
    "\n",
    "1. **Guarantees the dataset is properly loaded**  \n",
    "   Prevents errors during feature-selection or model training.\n",
    "\n",
    "2. **Automatically identifies label columns**  \n",
    "   Many astronomical datasets do not have explicit labels; this helps locate them reliably.\n",
    "\n",
    "3. **Highlights astrophysically meaningful features**  \n",
    "   Prevents important scientific variables from being accidentally removed.\n",
    "\n",
    "4. **Supports transparent and explainable ML pipelines**  \n",
    "   By identifying key features early, the researcher can justify:\n",
    "   - which features are included  \n",
    "   - which ones are removed  \n",
    "   - why certain columns matter scientifically  \n",
    "\n",
    "5. **Creates a structured foundation**  \n",
    "   All following steps‚Äîvariance filtering, correlation analysis, clustering, supervised learning‚Äîdepend on correctly identifying these columns.\n",
    "\n",
    "Overall, this cell ensures a scientifically grounded and well-structured start to the feature-selection process.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ed32c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (62368, 42)\n",
      "Detected possible targets: []\n",
      "Astrophysical priority columns found: ['ra', 'dec', 'filtercode', 'obsdate', 'obsjd', 'filefracday', 'seeing', 'airmass', 'maglimit', 'ra1', 'dec1', 'ra2', 'dec2', 'ra3', 'dec3', 'ra4', 'dec4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62368, 42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and load standardized dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = 'ztf_image_search_results_full_standardized.csv'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f'{DATA_PATH} not found. Run the preprocessing/standardization notebook first.')\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded dataset shape:', df.shape)\n",
    "# helper to detect likely target names and astrophysical important columns\n",
    "possible_targets = [c for c in df.columns if c.lower() in ['label','class','target','type']]\n",
    "astro_priority = [c for c in df.columns if any(k in c.lower() for k in ['ra','dec','flux','mag','snr','seeing','airmass','filter','band','jd','obsdate','mjd','maglimit'])]\n",
    "print('Detected possible targets:', possible_targets)\n",
    "print('Astrophysical priority columns found:', astro_priority)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d57c9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Removing Features With Excessive Missing Values (>90% Missing)</summary>\n",
    "\n",
    "This cell performs an important early step in feature selection by removing any columns (features) that contain **more than 90% missing values**. In practice, features with extremely high missingness provide little to no useful information for machine-learning models or scientific interpretation. Keeping such columns can introduce noise, distort statistical patterns, or cause algorithms to fail.\n",
    "\n",
    "The cell calculates the fraction of missing values in each column, identifies those with too much missingness, drops them from the dataset, and reports how many were removed. This ensures the remaining dataset contains features that are informative, reliable, and suitable for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Computes the missing-value percentage** for every column.  \n",
    "- **Keeps only columns with at least 10% valid data** (i.e., less than 90% missing).  \n",
    "- **Drops extremely sparse features**, which typically do not contribute meaningful information.  \n",
    "- **Prints the names of removed columns**, allowing transparency in the feature-selection process.  \n",
    "- **Updates and prints the new dataset shape**, showing how many features remain.  \n",
    "- Helps prevent:\n",
    "  - unreliable model training  \n",
    "  - statistical distortions  \n",
    "  - unnecessary dimensionality  \n",
    "  - increased computational cost  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Improves data quality**  \n",
    "   Features with too many missing values cannot reliably support scientific conclusions or machine-learning predictions.\n",
    "\n",
    "2. **Reduces dimensionality early**  \n",
    "   Removing uninformative features simplifies the dataset and improves computational efficiency.\n",
    "\n",
    "3. **Prevents model instability**  \n",
    "   ML algorithms struggle with columns that contain predominantly empty or imputed values.\n",
    "\n",
    "4. **Enhances interpretability**  \n",
    "   Keeping only meaningful columns helps focus the analysis on scientifically relevant variables.\n",
    "\n",
    "5. **Standard practice in data science**  \n",
    "   Dropping features with >90% missingness is widely used to ensure clean, analyzable datasets.\n",
    "\n",
    "This step ensures the dataset entering deeper feature-selection methods is clean, structured, and scientifically valid.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3165cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 columns with >90% missing:  []\n",
      "Shape after missingness drop: (62368, 42)\n"
     ]
    }
   ],
   "source": [
    "# 1) Drop features with >90% missing values\n",
    "thresh = 0.1  # keep columns with at least 10% non-missing\n",
    "missing_frac = df.isnull().mean()\n",
    "cols_keep = missing_frac[missing_frac <= (1 - thresh)].index.tolist()\n",
    "dropped_missing = [c for c in df.columns if c not in cols_keep]\n",
    "print(f'Dropping {len(dropped_missing)} columns with >90% missing: ', dropped_missing)\n",
    "df = df[cols_keep].copy()\n",
    "print('Shape after missingness drop:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5583f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Basic Imputation for Handling Remaining Missing Values</summary>\n",
    "\n",
    "This cell fills in (imputes) any remaining missing values in the dataset. Even after removing columns with excessive missingness, some features still contain gaps. Machine-learning models cannot work with missing values directly, so they must be replaced with reasonable estimates.\n",
    "\n",
    "The approach used here is simple, reliable, and widely accepted:\n",
    "\n",
    "- **Numeric features** ‚Üí replaced with the **median**  \n",
    "- **Categorical features** ‚Üí replaced with the **mode** (most frequent category)\n",
    "\n",
    "Median imputation avoids being influenced by extreme values, while mode imputation preserves the most common category. These strategies ensure that the dataset remains statistically consistent without introducing unrealistic values.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Identifies numeric and categorical columns** separately.  \n",
    "- **Numeric columns**: Missing values filled with the **median**, which is stable and unaffected by outliers.  \n",
    "- **Categorical columns**: Missing values filled with the **most frequent category** (mode).  \n",
    "- Handles rare cases where a mode does not exist by inserting `\"missing\"`.  \n",
    "- Ensures that **all remaining missing values are removed**.  \n",
    "- **Prints the top 10 columns with any remaining missingness**, confirming successful imputation.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Machine-learning algorithms require complete data**  \n",
    "   Missing values must be filled before training or evaluation.\n",
    "\n",
    "2. **Chosen methods preserve statistical behavior**  \n",
    "   - Median protects numeric distributions from outliers.  \n",
    "   - Mode maintains category consistency.\n",
    "\n",
    "3. **Prevents bias and errors**  \n",
    "   Proper imputation avoids artificial patterns that could mislead models.\n",
    "\n",
    "4. **Ensures fairness in feature selection**  \n",
    "   Columns are not dropped unnecessarily simply because they contain a few missing values.\n",
    "\n",
    "5. **Improves model stability and reliability**  \n",
    "   Clean, complete data is essential for robust scientific and ML results.\n",
    "\n",
    "By performing imputation at this stage, the dataset becomes **fully usable**, enabling downstream tasks such as variance filtering, clustering, supervised learning, and astrophysical analysis.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc345df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation, missing per column (top 10):\n",
      "ra            0\n",
      "dec           0\n",
      "infobits      0\n",
      "field         0\n",
      "ccdid         0\n",
      "qid           0\n",
      "rcid          0\n",
      "fid           0\n",
      "filtercode    0\n",
      "pid           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2) Basic imputation: numeric -> median, categorical -> mode\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "for c in num_cols:\n",
    "    if df[c].isnull().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "for c in cat_cols:\n",
    "    if df[c].isnull().any():\n",
    "        df[c] = df[c].fillna(df[c].mode().iloc[0] if not df[c].mode().empty else 'missing')\n",
    "print('After imputation, missing per column (top 10):')\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50861bd6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Removing Low-Variance (Near-Constant) Features</summary>\n",
    "\n",
    "This cell removes **low-variance features**, which are columns whose values are almost identical across all observations. Such features offer little or no useful information for machine-learning models because they do not help differentiate one sample from another.\n",
    "\n",
    "For example, if a column has nearly the same value for every astronomical observation, it cannot contribute to predicting object types or identifying meaningful patterns. Removing these features reduces noise, speeds up computation, and improves model performance.\n",
    "\n",
    "The `VarianceThreshold` tool identifies which numeric columns vary enough to be informative. Columns with extremely tiny variance (less than 1e-5) are dropped, and the dataset is rebuilt using only meaningful numeric and categorical features.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Selects all numeric columns** and evaluates their variance.  \n",
    "- **Drops features that are nearly constant**, since they carry little information.  \n",
    "- **Prints the names of removed low-variance features** for transparency.  \n",
    "- **Rebuilds the dataset** using:\n",
    "  - the remaining numeric features  \n",
    "  - all original categorical features  \n",
    "- **Updates and prints the new dataset shape** after filtering.  \n",
    "- **Improves efficiency** by reducing dimensionality.  \n",
    "- Helps prevent:\n",
    "  - redundant information  \n",
    "  - unnecessary computational cost  \n",
    "  - model confusion from uninformative features  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Eliminates uninformative features**  \n",
    "   Near-constant columns cannot help ML models distinguish between different objects or observations.\n",
    "\n",
    "2. **Enhances model performance**  \n",
    "   Removing noise improves accuracy, stability, and training speed.\n",
    "\n",
    "3. **Reduces dimensionality**  \n",
    "   Leaner datasets make feature selection and algorithm performance more efficient.\n",
    "\n",
    "4. **Improves interpretability**  \n",
    "   A dataset with fewer, more meaningful features is easier to analyze and explain.\n",
    "\n",
    "5. **Standard practice in feature engineering**  \n",
    "   Low-variance filtering is a widely used first step in preparing structured datasets for ML.\n",
    "\n",
    "This cell ensures that the dataset only contains features that meaningfully contribute to astronomical classification or pattern discovery.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487e3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-variance removed: ['field', 'itid', 'moonesb', 'crpix1', 'crpix2']\n",
      "Shape after low-variance filter: (62368, 37)\n"
     ]
    }
   ],
   "source": [
    "# 3) Low variance filter (remove near-constant features)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "num_df = df.select_dtypes(include=['number']).copy()\n",
    "if num_df.shape[1] > 0:\n",
    "    selector_var = VarianceThreshold(threshold=1e-5)\n",
    "    selector_var.fit(num_df)\n",
    "    keep_mask = selector_var.get_support()\n",
    "    lowvar_removed = [col for i,col in enumerate(num_df.columns) if not keep_mask[i]]\n",
    "    print('Low-variance removed:', lowvar_removed)\n",
    "    num_df = num_df.loc[:, keep_mask]\n",
    "    # rebuild df with remaining numeric cols + categorical cols\n",
    "    df = pd.concat([num_df.reset_index(drop=True), df[cat_cols].reset_index(drop=True)], axis=1)\n",
    "    print('Shape after low-variance filter:', df.shape)\n",
    "else:\n",
    "    print('No numeric columns for variance filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bdf66b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Removing Highly Correlated Features (r > 0.95)</summary>\n",
    "\n",
    "This cell removes **highly correlated numeric features**, which are pairs of columns that carry almost the same information. When two features have an extremely strong correlation (above 0.95), one of them becomes redundant. Keeping both adds unnecessary dimensionality, can mislead machine-learning models, and may cause overfitting.\n",
    "\n",
    "To fix this, the cell computes a correlation matrix, identifies pairs with correlation > 0.95, and removes one feature from each pair. This ensures that only unique, non-redundant information remains in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Computes the absolute correlation matrix** for all numeric features.  \n",
    "- Focuses on the **upper triangle** of the matrix to avoid duplicate comparisons.  \n",
    "- Identifies columns with correlations above **0.95**, meaning they carry almost identical information.  \n",
    "- **Removes redundant features**, keeping only one representative from each correlated pair.  \n",
    "- Rebuilds the dataset using:\n",
    "  - the remaining (non-redundant) numeric features  \n",
    "  - all original categorical features  \n",
    "- **Prints the number of features removed** and the updated dataset shape.  \n",
    "- Prevents model issues such as:\n",
    "  - multicollinearity  \n",
    "  - overfitting  \n",
    "  - unstable coefficient estimates (in linear models)  \n",
    "  - inflated feature importance scores  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Improves machine-learning model performance**  \n",
    "   Models become more stable and accurate when correlated noise is removed.\n",
    "\n",
    "2. **Reduces dimensionality efficiently**  \n",
    "   Removes unnecessary features without losing meaningful information.\n",
    "\n",
    "3. **Prevents multicollinearity**  \n",
    "   Essential for linear models like Logistic Regression where correlated predictors cause instability.\n",
    "\n",
    "4. **Improves interpretability**  \n",
    "   A cleaner set of independent features is easier to analyze and explain scientifically.\n",
    "\n",
    "5. **Scientifically meaningful**  \n",
    "   Many astronomical features may be derived from the same measurements (e.g., brightness and SNR), so removing redundant features prevents duplication of similar astrophysical signals.\n",
    "\n",
    "This step ensures the dataset contains a **compact, non-redundant set of features**, improving both the scientific clarity and the ML readiness of the dataset.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8640e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation-based drop count: 18\n",
      "Shape after correlation pruning: (62368, 19)\n"
     ]
    }
   ],
   "source": [
    "# 4) Correlation-based removal: remove one of each highly-correlated pair (r>0.95)\n",
    "num_df = df.select_dtypes(include=['number']).copy()\n",
    "corr_matrix = num_df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones_like(corr_matrix), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print('Correlation-based drop count:', len(to_drop_corr))\n",
    "num_df = num_df.drop(columns=to_drop_corr)\n",
    "df = pd.concat([num_df.reset_index(drop=True), df[cat_cols].reset_index(drop=True)], axis=1)\n",
    "print('Shape after correlation pruning:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411cc8c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Preparing Feature Matrix (X) and Target Labels (y), with Optional KMeans Proxy Labels</summary>\n",
    "\n",
    "This cell prepares the dataset for machine-learning tasks by constructing:\n",
    "\n",
    "- **X** ‚Üí the feature matrix (all predictor variables)  \n",
    "- **y** ‚Üí the target labels (the values to be predicted)\n",
    "\n",
    "In supervised learning, a label column must exist. However, astronomical datasets often come **without labeled classes**, since many objects are unlabeled or their physical types are unknown. To handle both labeled and unlabeled cases, the cell includes a fallback strategy:\n",
    "\n",
    "- If a true label column exists (e.g., ‚Äúclass‚Äù, ‚Äútype‚Äù), it is used directly as **y**.  \n",
    "- If no label is present, the cell **creates proxy labels** using **KMeans clustering**, grouping the data into 3 clusters based on feature similarity.\n",
    "\n",
    "This allows downstream feature-selection, classification, and evaluation techniques to work even when the dataset has no ground-truth labels‚Äîan extremely common scenario in astronomy.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Searches for a target/label column** with common names such as:\n",
    "  - label  \n",
    "  - class  \n",
    "  - target  \n",
    "  - type  \n",
    "- If found:\n",
    "  - Uses it as **y** (the target variable).  \n",
    "  - Converts categorical labels to numeric values using **LabelEncoder**.  \n",
    "- If **no target exists**:\n",
    "  - Prints a warning to indicate the dataset is unlabeled.  \n",
    "  - Uses **KMeans clustering** to automatically group observations into 3 clusters.  \n",
    "  - These cluster assignments act as **proxy labels (y)** for feature-selection experiments.  \n",
    "- **X** is created by removing the target column (if one exists) or keeping all features otherwise.  \n",
    "- Outputs the shapes of X and y to confirm correct construction.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Supports both labeled and unlabeled datasets**  \n",
    "   Many astronomical surveys (including ZTF) lack consistent object classifications. This cell provides a universal approach.\n",
    "\n",
    "2. **Enables feature selection in unlabeled settings**  \n",
    "   Proxy labels allow supervised feature-selection methods (Random Forest, Mutual Information, etc.) to operate even without real labels.\n",
    "\n",
    "3. **Provides scientifically meaningful structure**  \n",
    "   KMeans clustering treats similar observations as belonging to the same group‚Äîuseful when true labels are unknown.\n",
    "\n",
    "4. **Ensures compatibility with downstream ML steps**  \n",
    "   Models require numeric labels; this cell guarantees that **y is always numeric and valid**.\n",
    "\n",
    "5. **Encourages exploration**  \n",
    "   Proxy labels help identify natural patterns in the dataset before formal classification models are built.\n",
    "\n",
    "This step bridges the gap between raw standardized data and machine-learning readiness, enabling both supervised and unsupervised analyses in astronomical contexts.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9082d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No labeled target found; creating KMeans-based proxy labels\n",
      "X shape, y length: (62368, 19) 62368\n"
     ]
    }
   ],
   "source": [
    "# 5) Prepare X, y. If no target exists, create KMeans cluster labels as proxy target\n",
    "possible_targets = [c for c in df.columns if c.lower() in ['label','class','target','type']]\n",
    "target_col = possible_targets[0] if possible_targets else None\n",
    "if target_col and target_col in df.columns:\n",
    "    y = df[target_col].copy()\n",
    "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y.astype(str))\n",
    "    X = df.drop(columns=[target_col])\n",
    "    print('Using provided target column:', target_col)\n",
    "else:\n",
    "    print('No labeled target found; creating KMeans-based proxy labels')\n",
    "    X = df.copy()\n",
    "    X_num = X.select_dtypes(include=['number']).fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_num)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    y = kmeans.fit_predict(X_scaled)\n",
    "print('X shape, y length:', X.shape, len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892f2ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Preparing Target Labels (y) and Feature Matrix (X) ‚Äî Using True Labels or Creating Proxy Labels with KMeans</summary>\n",
    "\n",
    "This cell prepares the input features (**X**) and the corresponding target labels (**y**) for machine-learning experiments. Many astronomical datasets either lack labeled targets or include labels with inconsistent formats. To allow both supervised and unsupervised workflows, this cell handles both possibilities:\n",
    "\n",
    "1. **If a true target label exists** (e.g., ‚Äúlabel‚Äù, ‚Äúclass‚Äù, ‚Äútype‚Äù):  \n",
    "   - It uses that column as **y**.  \n",
    "   - Converts categorical labels to numeric form using **LabelEncoder**.  \n",
    "\n",
    "2. **If no target column exists**:  \n",
    "   - The cell **automatically creates proxy labels** using **KMeans clustering**.  \n",
    "   - These cluster labels approximate natural groupings in the dataset.  \n",
    "   - This enables experimentation with ML pipelines even without manually annotated data.\n",
    "\n",
    "This flexible approach allows the researcher to perform classification-like tasks even in unlabeled datasets‚Äîvery common in astronomy.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Key Points (Simple & Attractive Explanation)**\n",
    "\n",
    "- **Searches for a real target column** using common names such as:\n",
    "  - \"label\", \"class\", \"target\", \"type\"  \n",
    "- If found:\n",
    "  - Extracts it as **y**  \n",
    "  - Encodes text categories into numeric labels for ML compatibility  \n",
    "  - Defines **X** as all remaining features  \n",
    "- If *no* label is found:\n",
    "  - Prints a message indicating no target exists  \n",
    "  - Creates **unsupervised cluster labels** using KMeans (3 clusters)  \n",
    "  - Normalizes numeric data with StandardScaler before clustering  \n",
    "  - Uses the resulting cluster index as a **proxy label y**  \n",
    "- Finally prints the shapes of **X** and **y**, confirming readiness for further steps.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚≠ê **Why This Cell Is Important for the Research**\n",
    "\n",
    "1. **Provides flexibility for labeled and unlabeled datasets**  \n",
    "   Astronomical datasets often lack manually labeled classes. This method ensures ML can still proceed.\n",
    "\n",
    "2. **Enables supervised learning experiments even without true labels**  \n",
    "   KMeans proxy labels allow baseline model evaluation, feature selection, and representation learning.\n",
    "\n",
    "3. **Supports contrastive and semi-supervised workflows**  \n",
    "   Proxy labels are particularly valuable for:\n",
    "   - pretraining  \n",
    "   - representation evaluation  \n",
    "   - clustering validity checks  \n",
    "\n",
    "4. **Ensures ML models receive properly formatted data**  \n",
    "   - Categorical labels are encoded  \n",
    "   - Numeric features are scaled for clustering  \n",
    "\n",
    "5. **Maintains scientific integrity**  \n",
    "   The clustering method is applied only to standardized numeric features, preserving the statistical structure of the data.\n",
    "\n",
    "Overall, this cell bridges the gap between raw astronomical data and practical machine-learning workflows by ensuring that both labeled and unlabeled datasets are usable for model development.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40049ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SelectKBest mutual_info (k= 14 ) on numeric features\n",
      "SelectKBest selected: ['ra', 'dec', 'infobits', 'qid', 'fid', 'pid', 'exptime', 'seeing', 'airmass', 'moonillf', 'maglimit', 'cd11', 'cd22', 'ipac_gid']\n",
      "RandomForest top features: ['ipac_gid', 'cd11', 'cd22', 'pid', 'fid', 'airmass', 'maglimit', 'ra', 'moonillf', 'dec', 'seeing', 'exptime', 'infobits', 'qid']\n",
      "L1-selected features (non-zero): []\n",
      "PCA top features: ['infobits', 'qid', 'exptime', 'cd22', 'airmass', 'pid', 'maglimit', 'moonillf', 'cd11', 'seeing', 'fid', 'ipac_gid', 'ra', 'dec']\n",
      "Ranked features by method votes (top 30): ['ra', 'dec', 'infobits', 'qid', 'fid', 'pid', 'exptime', 'seeing', 'airmass', 'moonillf', 'maglimit', 'cd11', 'cd22', 'ipac_gid']\n"
     ]
    }
   ],
   "source": [
    "# 6) Supervised/Proxy selection methods\\n# 6a) SelectKBest with mutual_info_classif (works with discrete y)\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "k = min(20, max(1, len(num_cols)))\n",
    "print('Running SelectKBest mutual_info (k=', k, ') on numeric features')\n",
    "skb_selected = []\n",
    "if len(num_cols) > 0:\n",
    "    skb = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_num = X[num_cols].fillna(0)\n",
    "    try:\n",
    "        skb.fit(X_num, y)\n",
    "        skb_selected = [f for f, s in zip(num_cols, skb.get_support()) if s]\n",
    "        print('SelectKBest selected:', skb_selected)\n",
    "    except Exception as e:\n",
    "        print('SelectKBest failed:', e)\n",
    "else:\n",
    "    print('No numeric features for SelectKBest')\n",
    "\n",
    "# 6b) RandomForest feature importance\n",
    "rf_selected = []\n",
    "try:\n",
    "    if len(num_cols) > 0:\n",
    "        rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_num, y)\n",
    "        importances = pd.Series(rf.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
    "        rf_selected = importances.head(k).index.tolist()\n",
    "        print('RandomForest top features:', rf_selected)\n",
    "    else:\n",
    "        print('No numeric features for RandomForest')\n",
    "except Exception as e:\n",
    "    print('RandomForest failed:', e)\n",
    "\n",
    "# 6c) L1-based selection (LogisticRegression with L1) - only for classification targets\n",
    "l1_selected = []\n",
    "try:\n",
    "    if len(num_cols) > 0:\n",
    "        lr = LogisticRegression(penalty='l1', solver='saga', max_iter=5000, random_state=42)\n",
    "        lr.fit(X_num, y)\n",
    "        coef = np.abs(lr.coef_).sum(axis=0) if lr.coef_.ndim > 1 else np.abs(lr.coef_)\n",
    "        coef_series = pd.Series(coef, index=num_cols).sort_values(ascending=False)\n",
    "        l1_selected = coef_series[coef_series > 1e-6].index.tolist()\n",
    "        print('L1-selected features (non-zero):', l1_selected[:k])\n",
    "    else:\n",
    "        print('No numeric features for L1 selection')\n",
    "except Exception as e:\n",
    "    print('L1 selection failed:', e)\n",
    "\n",
    "# 6d) PCA loadings: features with largest absolute loadings on first components\n",
    "from sklearn.decomposition import PCA\n",
    "pca_selected = []\n",
    "try:\n",
    "    if len(num_cols) > 0:\n",
    "        pca = PCA(n_components=min(6, len(num_cols)))\n",
    "        Xp = pca.fit_transform(X_num)\n",
    "        loadings = np.abs(pca.components_).sum(axis=0)\n",
    "        loadings_series = pd.Series(loadings, index=num_cols).sort_values(ascending=False)\n",
    "        pca_selected = loadings_series.head(k).index.tolist()\n",
    "        print('PCA top features:', pca_selected)\n",
    "    else:\n",
    "        print('No numeric features for PCA')\n",
    "except Exception as e:\n",
    "    print('PCA failed:', e)\n",
    "\n",
    "# Consolidate selections into a ranking count\n",
    "from collections import Counter\n",
    "all_methods = [tuple(skb_selected), tuple(rf_selected), tuple(l1_selected), tuple(pca_selected)]\n",
    "flat = [f for method in all_methods for f in method]\n",
    "counts = Counter(flat)\n",
    "ranked = [f for f, _ in counts.most_common()]\n",
    "print('Ranked features by method votes (top 30):', ranked[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99fec1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Domain-Aware Final Feature Selection (Astrophysics + ML Consensus)</summary>\n",
    "\n",
    "This cell performs the final and most important stage of feature selection by **combining machine-learning consensus rules with astrophysics domain knowledge**.  \n",
    "Instead of relying only on statistical filters, we ensure that all features scientifically important for astronomical behavior (RA, Dec, flux, magnitude, SNR, airmass, filter band, etc.) are forcibly retained if present.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚≠ê What This Cell Does (Attractive Point-Wise Description)\n",
    "\n",
    "### **1Ô∏è‚É£ Detect and preserve astrophysically meaningful features**\n",
    "It scans all column names to find domain-critical features such as:\n",
    "- **RA, Dec** ‚Üí celestial coordinates  \n",
    "- **flux, mag** ‚Üí brightness measurements  \n",
    "- **SNR (Signal-to-Noise Ratio)**  \n",
    "- **seeing, airmass** ‚Üí observational conditions  \n",
    "- **filter/band** ‚Üí photometric channel  \n",
    "- **maglimit** ‚Üí limiting magnitude of observation  \n",
    "\n",
    "These are added to a **priority list** and will NEVER be dropped.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Build a unified selection strategy using ML + Domain Rules**\n",
    "This combines results from earlier steps:\n",
    "- Variance threshold  \n",
    "- Correlation pruning  \n",
    "- SelectKBest (MI)  \n",
    "- Random Forest importance  \n",
    "- PCA ranking  \n",
    "\n",
    "A feature is kept if:\n",
    "\n",
    "‚úî Selected by **‚â•2 methods** (ML consensus)  \n",
    "**OR**  \n",
    "‚úî It is an **astronomy-priority feature**\n",
    "\n",
    "This ensures the final feature set is both **predictive** and **scientifically meaningful**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Handles edge cases safely**\n",
    "If no feature satisfies the rule (rare case), it falls back to:\n",
    "- Top Random Forest features  \n",
    "- Top PCA features  \n",
    "\n",
    "This guarantees the model always receives a usable feature set.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Combine, reorder, and limit final list**\n",
    "The final selected features are:\n",
    "- Ordered according to earlier ranking  \n",
    "- Completed with additional priority features if missing  \n",
    "- **Capped at 30 features** to prevent overfitting and maintain efficiency  \n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Print the final list for downstream modeling**\n",
    "This list is then used in:\n",
    "- Supervised learning  \n",
    "- Contrastive learning  \n",
    "- Clustering  \n",
    "- Representation evaluation  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why This Step Is Scientifically Strong\n",
    "\n",
    "- ‚ú® **ML alone cannot judge scientific importance**  \n",
    "  Example: RA/Dec might have low variance or high correlation but are essential to preserve.\n",
    "\n",
    "- ‚ú® **Domain knowledge prevents accidental loss of astrophysical meaning**  \n",
    "  Flux + magnitude + SNR are fundamental for transient classification.\n",
    "\n",
    "- ‚ú® **Hybrid selection ensures generalization**  \n",
    "  Consensus among multiple selection techniques reduces noise features.\n",
    "\n",
    "- ‚ú® **Feature cap prevents the curse of dimensionality**  \n",
    "  Very important for models like Random Forests, SVMs, or contrastive encoders.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Final Statement (for your thesis/report)\n",
    "You may include:\n",
    "\n",
    "> ‚ÄúTo ensure scientifically grounded feature selection, we combined machine-learning consensus (variance filtering, correlation pruning, mutual information, Random Forest importance, and PCA ranking) with astronomy domain knowledge. Any feature selected by at least two ML methods or identified as astrophysically essential (RA, Dec, flux, magnitude, SNR, airmass, filters, etc.) was preserved. The final list was capped at 30 features to balance representational power and model complexity.‚Äù\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af54601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority features to preserve (if present): ['ra', 'dec', 'seeing', 'airmass', 'maglimit', 'filtercode']\n",
      "Final selected features (count=15): ['ra', 'dec', 'infobits', 'qid', 'fid', 'pid', 'exptime', 'seeing', 'airmass', 'moonillf', 'maglimit', 'cd11', 'cd22', 'ipac_gid', 'filtercode']\n"
     ]
    }
   ],
   "source": [
    "# 7) Apply astronomy domain knowledge: ensure astrophysical features are kept if present\n",
    "priority = [c for c in df.columns if any(k in c.lower() for k in ['ra','dec','flux','mag','snr','seeing','airmass','maglimit','filter','band'])]\n",
    "print('Priority features to preserve (if present):', priority)\n",
    "# Final selection strategy: take features selected by at least two methods OR in priority list. Limit to 30 features max.\n",
    "selected_set = set()\n",
    "for f, cnt in counts.items():\n",
    "    if cnt >= 2:\n",
    "        selected_set.add(f)\n",
    "# add priority features\n",
    "for p in priority:\n",
    "    if p in df.columns:\n",
    "        selected_set.add(p)\n",
    "# If selection is empty (edge cases), fall back to top RF features or top PCA\n",
    "if len(selected_set) == 0:\n",
    "    selected_set.update(rf_selected[:min(20, len(rf_selected))])\n",
    "selected_list = [f for f in ranked if f in selected_set]\n",
    "# append any priority features not in ranked at the end\n",
    "for p in priority:\n",
    "    if p in df.columns and p not in selected_list:\n",
    "        selected_list.append(p)\n",
    "# limit to 30\n",
    "selected_list = selected_list[:30]\n",
    "print('Final selected features (count={}):'.format(len(selected_list)), selected_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467466df",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìå Cell Description: Saving the Final Selected Features (Clean Export for Modeling)</summary>\n",
    "\n",
    "This cell finalizes the feature-selection pipeline by **exporting only the best and most scientifically meaningful features** into clean, reusable files.  \n",
    "It ensures that the dataset handed to machine learning models contains only high-quality inputs that support accurate astronomical predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚≠ê What This Cell Does (Attractive Point-Wise Description)\n",
    "\n",
    "### **1Ô∏è‚É£ Starts with the final refined feature list**  \n",
    "The cell takes the previously selected features (`selected_list`) and prepares them for export.  \n",
    "This includes:\n",
    "- ML-selected features  \n",
    "- Domain-preserved astronomy features  \n",
    "- Filtered, deduplicated, and ranked attributes  \n",
    "\n",
    "This stage ensures only strong, validated inputs are kept.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Removes irrelevant or technical columns**  \n",
    "Some dataset columns (e.g., `pid`, `filtercode`) do not contribute to astronomy or data science tasks.  \n",
    "They are:\n",
    "- identifiers  \n",
    "- unnecessary system codes  \n",
    "- not useful for model learning  \n",
    "\n",
    "These are safely removed to avoid noise and improve model clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Adds the target/label column when available**  \n",
    "If a target column (such as *class*, *type*, *label*) exists, it is prepended to the feature list.  \n",
    "This is essential because:\n",
    "- ML models need access to the label for training  \n",
    "- Keeping it ensures correct dataset structure  \n",
    "\n",
    "This step ensures smooth downstream learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Validates that all selected columns actually exist in the DataFrame**  \n",
    "Some features may have been removed earlier due to:\n",
    "- missing values  \n",
    "- cleaning steps  \n",
    "- correlation pruning  \n",
    "\n",
    "This validation prevents errors and ensures the final dataset is usable and consistent.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Blocks accidental empty selections**  \n",
    "If‚Äîafter cleaning‚Äîthe list becomes empty, the cell stops and raises an error.  \n",
    "This is a safety guard that ensures the research process never continues with an invalid dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ Saves the final curated dataset to CSV**  \n",
    "A compact dataset is written to:\n",
    "\n",
    "üìÑ **`ztf_selected_features.csv`**\n",
    "\n",
    "This file contains:\n",
    "- only the final scientifically-validated features  \n",
    "- plus the target column (if present)\n",
    "\n",
    "This clean dataset is ready for:\n",
    "- machine learning  \n",
    "- deep learning  \n",
    "- visual analysis  \n",
    "- contrastive learning experiments  \n",
    "\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ Writes a simple text file listing the selected features**  \n",
    "A second file is created:\n",
    "\n",
    "üìÑ **`selected_feature_list.txt`**\n",
    "\n",
    "It contains:\n",
    "- one feature name per line  \n",
    "- no target column (to avoid confusion)  \n",
    "\n",
    "This is helpful for:\n",
    "- documentation  \n",
    "- replication by other researchers  \n",
    "- explaining feature importance in viva or thesis  \n",
    "\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ Shows a preview for verification**  \n",
    "A short printout allows you to quickly inspect:\n",
    "- the saved columns  \n",
    "- the appearance of the exported dataset  \n",
    "\n",
    "This acts as a final confirmation step.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why This Step Is Important in Data Science + Astronomy\n",
    "\n",
    "- ‚úî Ensures the dataset for modeling is **clean, compact, and optimized**  \n",
    "- ‚úî Removes unnecessary features, improving model accuracy and training speed  \n",
    "- ‚úî Preserves astronomy-required attributes, protecting scientific meaning  \n",
    "- ‚úî Produces reusable files to keep the research workflow organized  \n",
    "- ‚úî Makes it easy to share or re-run experiments consistently  \n",
    "\n",
    "This step guarantees that the final dataset reflects both **scientific understanding** and **data-science best practices**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Final Statement (for thesis/report)\n",
    "\n",
    "> ‚ÄúThe final selected features were exported into a compact dataset (`ztf_selected_features.csv`) and accompanied by a feature list file. All irrelevant identifiers were removed, and the target variable was preserved when present. This ensured a clean, model-ready dataset aligned with both machine-learning principles and astrophysical interpretability.‚Äù\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef078093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved selected features CSV -> ztf_selected_features.csv\n",
      "Saved feature list -> selected_feature_list.txt\n",
      "Example preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>infobits</th>\n",
       "      <th>qid</th>\n",
       "      <th>fid</th>\n",
       "      <th>exptime</th>\n",
       "      <th>seeing</th>\n",
       "      <th>airmass</th>\n",
       "      <th>moonillf</th>\n",
       "      <th>maglimit</th>\n",
       "      <th>cd11</th>\n",
       "      <th>cd22</th>\n",
       "      <th>ipac_gid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.149415</td>\n",
       "      <td>1.530960</td>\n",
       "      <td>3.271612</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.116146</td>\n",
       "      <td>-0.220519</td>\n",
       "      <td>0.195190</td>\n",
       "      <td>-1.552305</td>\n",
       "      <td>1.141877</td>\n",
       "      <td>0.455582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.541840</td>\n",
       "      <td>1.124622</td>\n",
       "      <td>-0.292168</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2.021751</td>\n",
       "      <td>-0.192858</td>\n",
       "      <td>-0.135013</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>1.205201</td>\n",
       "      <td>1.795089</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.550309</td>\n",
       "      <td>1.524426</td>\n",
       "      <td>-0.292168</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.320594</td>\n",
       "      <td>-0.712885</td>\n",
       "      <td>-0.093118</td>\n",
       "      <td>0.428065</td>\n",
       "      <td>2.258968</td>\n",
       "      <td>1.853579</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.542016</td>\n",
       "      <td>0.238707</td>\n",
       "      <td>-0.292168</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.971814</td>\n",
       "      <td>-0.718418</td>\n",
       "      <td>-1.855228</td>\n",
       "      <td>-0.023688</td>\n",
       "      <td>0.736894</td>\n",
       "      <td>1.647814</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.541997</td>\n",
       "      <td>0.237082</td>\n",
       "      <td>-0.292168</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2.016056</td>\n",
       "      <td>-0.707353</td>\n",
       "      <td>-2.049663</td>\n",
       "      <td>-2.488164</td>\n",
       "      <td>0.626096</td>\n",
       "      <td>1.476595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ra       dec  infobits  qid  fid  exptime    seeing   airmass  \\\n",
       "0 -1.149415  1.530960  3.271612    1    2       30 -0.116146 -0.220519   \n",
       "1 -1.541840  1.124622 -0.292168    3    2       30  2.021751 -0.192858   \n",
       "2 -1.550309  1.524426 -0.292168    2    2       30 -0.320594 -0.712885   \n",
       "3 -1.542016  0.238707 -0.292168    3    2       30 -0.971814 -0.718418   \n",
       "4 -1.541997  0.237082 -0.292168    3    1       30  2.016056 -0.707353   \n",
       "\n",
       "   moonillf  maglimit      cd11      cd22  ipac_gid  \n",
       "0  0.195190 -1.552305  1.141877  0.455582         2  \n",
       "1 -0.135013  0.080815  1.205201  1.795089         2  \n",
       "2 -0.093118  0.428065  2.258968  1.853579         2  \n",
       "3 -1.855228 -0.023688  0.736894  1.647814         3  \n",
       "4 -2.049663 -2.488164  0.626096  1.476595         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) Save selected features to CSV and a feature list text file\n",
    "out_csv = 'ztf_selected_features.csv'\n",
    "out_list = 'selected_feature_list.txt'\n",
    "\n",
    "# Make a copy of selected features\n",
    "keep_cols = selected_list.copy()\n",
    "\n",
    "# Remove unwanted columns\n",
    "cols_to_remove = ['pid', 'filtercode']\n",
    "keep_cols = [c for c in keep_cols if c not in cols_to_remove]\n",
    "\n",
    "# Keep target if present\n",
    "if target_col and target_col in df.columns:\n",
    "    keep_cols = [target_col] + keep_cols\n",
    "\n",
    "# Ensure columns exist in df\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "\n",
    "if len(keep_cols) == 0:\n",
    "    raise RuntimeError('No features selected ‚Äî check earlier steps')\n",
    "\n",
    "# Save CSV with only selected features (and target if present)\n",
    "df[keep_cols].to_csv(out_csv, index=False)\n",
    "\n",
    "# Save feature list (without target)\n",
    "with open(out_list, 'w') as fh:\n",
    "    for c in keep_cols:\n",
    "        if c != target_col:        # avoid writing the target twice\n",
    "            fh.write(c + '\\n')\n",
    "\n",
    "print('Saved selected features CSV ->', out_csv)\n",
    "print('Saved feature list ->', out_list)\n",
    "print('Example preview:')\n",
    "df[keep_cols].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
